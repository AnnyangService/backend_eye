import torch
import numpy as np
import logging
import json
import os
from datetime import datetime
from typing import List, Dict, Optional, Union
from transformers import AutoModel, AutoTokenizer
from flask import current_app

logger = logging.getLogger(__name__)

class RuleVectorizer:
    """
    KLUE RoBERTa ë˜ëŠ” KM-BERTë¥¼ ì‚¬ìš©í•˜ì—¬ ì§„ë‹¨ ë£°ì„ ë²¡í„°í™”í•˜ëŠ” í´ë˜ìŠ¤
    """
    
    def __init__(self, model_name: str = "klue/roberta-small", use_medical_model: bool = False):
        """
        RuleVectorizer ì´ˆê¸°í™”
        
        Args:
            model_name (str): ì‚¬ìš©í•  ëª¨ë¸ëª…
            use_medical_model (bool): ì˜ë£Œ íŠ¹í™” ëª¨ë¸ ì‚¬ìš© ì—¬ë¶€
        """
        if use_medical_model:
            self.model_name = "madatnlp/km-bert"
            self.tokenizer_name = "snunlp/KR-BERT-char16424"
            print("ğŸ¥ í•œêµ­ì–´ ì˜ë£Œ íŠ¹í™” KM-BERT ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        else:
            self.model_name = model_name
            self.tokenizer_name = model_name
            print("ğŸ¤– ì¼ë°˜ ì–¸ì–´ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        
        self.model = None
        self.tokenizer = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ëª¨ë¸ ë¡œë“œ
        self._load_model()
    
    def _load_model(self):
        """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            logger.info(f"ëª¨ë¸ ë¡œë“œ ì‹œì‘: {self.model_name}")
            logger.info(f"í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹œì‘: {self.tokenizer_name}")
            
            # í† í¬ë‚˜ì´ì € ë¡œë“œ (KM-BERTì˜ ê²½ìš° ë³„ë„ í† í¬ë‚˜ì´ì € ì‚¬ìš©)
            self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name)
            logger.info(f"í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ: {type(self.tokenizer).__name__}")
            
            # ëª¨ë¸ ë¡œë“œ
            self.model = AutoModel.from_pretrained(self.model_name)
            self.model = self.model.to(self.device)
            self.model.eval()
            
            logger.info(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ, ë””ë°”ì´ìŠ¤: {self.device}")
            
            # ëª¨ë¸ ì •ë³´ ì¶œë ¥
            total_params = sum(p.numel() for p in self.model.parameters())
            logger.info(f"ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,}")
            
            # ëª¨ë¸ íƒ€ì… ì¶œë ¥
            if "km-bert" in self.model_name.lower():
                logger.info("ğŸ¥ í•œêµ­ì–´ ì˜ë£Œ ë„ë©”ì¸ íŠ¹í™” ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            else:
                logger.info("ğŸ¤– ì¼ë°˜ ì–¸ì–´ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            raise Exception(f"RuleVectorizer ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
    
    def vectorize_text(self, text: str, max_length: int = 512) -> np.ndarray:
        """
        ë‹¨ì¼ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°í™”í•©ë‹ˆë‹¤.
        
        Args:
            text (str): ë²¡í„°í™”í•  í…ìŠ¤íŠ¸
            max_length (int): ìµœëŒ€ í† í° ê¸¸ì´
            
        Returns:
            np.ndarray: ë²¡í„°í™”ëœ ì„ë² ë”© (768ì°¨ì›)
        """
        try:
            if self.model is None or self.tokenizer is None:
                raise Exception("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            # í† í¬ë‚˜ì´ì§•
            inputs = self.tokenizer(
                text,
                return_tensors="pt",
                max_length=max_length,
                padding=True,
                truncation=True
            )
            
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # ì¶”ë¡  ì‹¤í–‰
            with torch.no_grad():
                outputs = self.model(**inputs)
                
                # [CLS] í† í°ì˜ ì„ë² ë”©ì„ ì‚¬ìš© (ì²« ë²ˆì§¸ í† í°)
                cls_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()
                
                # ë°°ì¹˜ ì°¨ì› ì œê±°í•˜ì—¬ 1ì°¨ì› ë²¡í„°ë¡œ ë°˜í™˜
                return cls_embedding.squeeze()
                
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ë²¡í„°í™” ì‹¤íŒ¨: {str(e)}")
            raise Exception(f"í…ìŠ¤íŠ¸ ë²¡í„°í™” ì‹¤íŒ¨: {str(e)}")
    
    def vectorize_texts(self, texts: List[str], max_length: int = 512) -> np.ndarray:
        """
        ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ë²¡í„°í™”í•©ë‹ˆë‹¤.
        
        Args:
            texts (List[str]): ë²¡í„°í™”í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            max_length (int): ìµœëŒ€ í† í° ê¸¸ì´
            
        Returns:
            np.ndarray: ë²¡í„°í™”ëœ ì„ë² ë”©ë“¤ (N x 768)
        """
        try:
            if self.model is None or self.tokenizer is None:
                raise Exception("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            if not texts:
                return np.array([])
            
            # í† í¬ë‚˜ì´ì§• (ë°°ì¹˜ ì²˜ë¦¬)
            inputs = self.tokenizer(
                texts,
                return_tensors="pt",
                max_length=max_length,
                padding=True,
                truncation=True
            )
            
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # ì¶”ë¡  ì‹¤í–‰
            with torch.no_grad():
                outputs = self.model(**inputs)
                
                # [CLS] í† í°ì˜ ì„ë² ë”©ì„ ì‚¬ìš©
                cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
                
                return cls_embeddings
                
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ë°°ì¹˜ ë²¡í„°í™” ì‹¤íŒ¨: {str(e)}")
            raise Exception(f"í…ìŠ¤íŠ¸ ë°°ì¹˜ ë²¡í„°í™” ì‹¤íŒ¨: {str(e)}")
    
    def vectorize_rule_description(self, rule_name: str, description: str, target_name: Optional[str] = None) -> Dict:
        """
        ì§„ë‹¨ ë£°ì˜ ì„¤ëª…ì„ ë²¡í„°í™”í•©ë‹ˆë‹¤.
        
        Args:
            rule_name (str): ë£° ì´ë¦„
            description (str): ë£° ì„¤ëª…
            target_name (Optional[str]): ì§„ë‹¨ ëŒ€ìƒ ì´ë¦„
            
        Returns:
            Dict: ë²¡í„°í™” ê²°ê³¼
        """
        try:
            # ì»¨í…ìŠ¤íŠ¸ê°€ ìˆëŠ” í…ìŠ¤íŠ¸ ìƒì„±
            if target_name:
                context_text = f"[{target_name}] {rule_name}: {description}"
            else:
                context_text = f"{rule_name}: {description}"
            
            # ë²¡í„°í™” ìˆ˜í–‰
            embedding = self.vectorize_text(context_text)
            
            # ê²°ê³¼ ë°˜í™˜
            result = {
                'rule_name': rule_name,
                'description': description,
                'target_name': target_name,
                'context_text': context_text,
                'embedding': embedding.tolist(),  # JSON ì§ë ¬í™”ë¥¼ ìœ„í•´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                'embedding_dimension': len(embedding),
                'model_name': self.model_name
            }
            
            logger.info(f"ë£° ë²¡í„°í™” ì™„ë£Œ: '{rule_name}' (ì°¨ì›: {len(embedding)})")
            return result
            
        except Exception as e:
            logger.error(f"ë£° ì„¤ëª… ë²¡í„°í™” ì‹¤íŒ¨: {str(e)}")
            raise Exception(f"ë£° ì„¤ëª… ë²¡í„°í™” ì‹¤íŒ¨: {str(e)}")
    
    def vectorize_multiple_rules(self, rules_data: List[Dict]) -> List[Dict]:
        """
        ì—¬ëŸ¬ ë£°ì„ ë°°ì¹˜ë¡œ ë²¡í„°í™”í•©ë‹ˆë‹¤.
        
        Args:
            rules_data (List[Dict]): ë£° ë°ì´í„° ë¦¬ìŠ¤íŠ¸
                ê° ë”•ì…”ë„ˆë¦¬ëŠ” 'rule_name', 'description', 'target_name'(ì„ íƒ) í‚¤ë¥¼ í¬í•¨
                
        Returns:
            List[Dict]: ë²¡í„°í™”ëœ ë£° ë¦¬ìŠ¤íŠ¸
        """
        try:
            if not rules_data:
                return []
            
            # ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸ ìƒì„±
            context_texts = []
            for rule_data in rules_data:
                rule_name = rule_data.get('rule_name', '')
                description = rule_data.get('description', '')
                target_name = rule_data.get('target_name')
                
                if target_name:
                    context_text = f"[{target_name}] {rule_name}: {description}"
                else:
                    context_text = f"{rule_name}: {description}"
                
                context_texts.append(context_text)
            
            # ë°°ì¹˜ ë²¡í„°í™”
            embeddings = self.vectorize_texts(context_texts)
            
            # ê²°ê³¼ ìƒì„±
            results = []
            for i, rule_data in enumerate(rules_data):
                result = {
                    'rule_name': rule_data.get('rule_name', ''),
                    'description': rule_data.get('description', ''),
                    'target_name': rule_data.get('target_name'),
                    'context_text': context_texts[i],
                    'embedding': embeddings[i].tolist(),
                    'embedding_dimension': len(embeddings[i]),
                    'model_name': self.model_name
                }
                results.append(result)
            
            logger.info(f"ë°°ì¹˜ ë£° ë²¡í„°í™” ì™„ë£Œ: {len(results)}ê°œ ë£° ì²˜ë¦¬")
            return results
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ë£° ë²¡í„°í™” ì‹¤íŒ¨: {str(e)}")
            raise Exception(f"ë°°ì¹˜ ë£° ë²¡í„°í™” ì‹¤íŒ¨: {str(e)}")
    
    def get_model_info(self) -> Dict:
        """ëª¨ë¸ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return {
            'model_name': self.model_name,
            'device': str(self.device),
            'model_loaded': self.model is not None,
            'tokenizer_loaded': self.tokenizer is not None,
            'tokenizer_type': type(self.tokenizer).__name__ if self.tokenizer else None,
            'max_position_embeddings': getattr(self.model.config, 'max_position_embeddings', None) if self.model else None,
            'hidden_size': getattr(self.model.config, 'hidden_size', None) if self.model else None
        }
    
    def cosine_similarity(self, embedding1: Union[np.ndarray, List], embedding2: Union[np.ndarray, List]) -> float:
        """
        ë‘ ì„ë² ë”© ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        
        Args:
            embedding1: ì²« ë²ˆì§¸ ì„ë² ë”©
            embedding2: ë‘ ë²ˆì§¸ ì„ë² ë”©
            
        Returns:
            float: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (-1 ~ 1)
        """
        try:
            # numpy ë°°ì—´ë¡œ ë³€í™˜
            vec1 = np.array(embedding1)
            vec2 = np.array(embedding2)
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            dot_product = np.dot(vec1, vec2)
            norm1 = np.linalg.norm(vec1)
            norm2 = np.linalg.norm(vec2)
            
            if norm1 == 0 or norm2 == 0:
                return 0.0
            
            return dot_product / (norm1 * norm2)
            
        except Exception as e:
            logger.error(f"ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {str(e)}")
            return 0.0

    def save_embedding_to_file(self, text: str, output_dir: str = "embeddings", filename: str = None):
        """
        í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°í™”í•˜ê³  íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
        
        Args:
            text (str): ë²¡í„°í™”í•  í…ìŠ¤íŠ¸
            output_dir (str): ì €ì¥í•  ë””ë ‰í† ë¦¬
            filename (str): íŒŒì¼ëª… (Noneì´ë©´ ìë™ ìƒì„±)
        
        Returns:
            Dict: ì €ì¥ ê²°ê³¼ ì •ë³´
        """
        try:
            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
            os.makedirs(output_dir, exist_ok=True)
            
            # ë²¡í„°í™” ìˆ˜í–‰
            embedding = self.vectorize_text(text)
            
            # íŒŒì¼ëª… ìƒì„±
            if filename is None:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                safe_text = "".join(c for c in text[:30] if c.isalnum() or c in (' ', '-', '_')).rstrip()
                safe_text = safe_text.replace(' ', '_')
                filename = f"embedding_{timestamp}_{safe_text}"
            
            # JSON íŒŒì¼ë¡œ ì €ì¥
            json_filepath = os.path.join(output_dir, f"{filename}.json")
            result_data = {
                'text': text,
                'embedding': embedding.tolist(),
                'embedding_dimension': len(embedding),
                'model_name': self.model_name,
                'device': str(self.device),
                'created_at': datetime.now().isoformat(),
                'text_length': len(text)
            }
            
            with open(json_filepath, 'w', encoding='utf-8') as f:
                json.dump(result_data, f, ensure_ascii=False, indent=2)
            
            # NumPy íŒŒì¼ë¡œë„ ì €ì¥ (ì„ë² ë”©ë§Œ)
            npy_filepath = os.path.join(output_dir, f"{filename}.npy")
            np.save(npy_filepath, embedding)
            
            # ê²°ê³¼ ì •ë³´
            result_info = {
                'success': True,
                'text': text,
                'text_length': len(text),
                'embedding_dimension': len(embedding),
                'json_file': json_filepath,
                'npy_file': npy_filepath,
                'model_name': self.model_name,
                'device': str(self.device)
            }
            
            print(f"âœ… ì„ë² ë”© ì €ì¥ ì™„ë£Œ!")
            print(f"   í…ìŠ¤íŠ¸: {text}")
            print(f"   ì°¨ì›: {len(embedding)}")
            print(f"   JSON íŒŒì¼: {json_filepath}")
            print(f"   NumPy íŒŒì¼: {npy_filepath}")
            
            return result_info
            
        except Exception as e:
            error_msg = f"ì„ë² ë”© íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}"
            logger.error(error_msg)
            return {
                'success': False,
                'error': error_msg,
                'text': text
            }

    def analyze_text_tokens(self, text: str) -> Dict:
        """
        í…ìŠ¤íŠ¸ì˜ í† í° ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            text (str): ë¶„ì„í•  í…ìŠ¤íŠ¸
            
        Returns:
            Dict: í† í° ë¶„ì„ ê²°ê³¼
        """
        try:
            if self.tokenizer is None:
                raise Exception("í† í¬ë‚˜ì´ì €ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            # í† í¬ë‚˜ì´ì§•
            inputs = self.tokenizer(
                text,
                return_tensors="pt",
                max_length=512,
                padding=True,
                truncation=True,
                return_attention_mask=True,
                return_token_type_ids=False
            )
            
            # í† í° IDë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            token_ids = inputs['input_ids'][0].tolist()
            tokens = self.tokenizer.convert_ids_to_tokens(token_ids)
            
            # íŠ¹ìˆ˜ í† í° ì œê±°í•œ ì‹¤ì œ í† í°ë“¤
            actual_tokens = [token for token in tokens if token not in ['[CLS]', '[SEP]', '[PAD]']]
            
            return {
                'original_text': text,
                'token_ids': token_ids,
                'tokens': tokens,
                'actual_tokens': actual_tokens,
                'token_count': len(actual_tokens),
                'attention_mask': inputs['attention_mask'][0].tolist()
            }
            
        except Exception as e:
            logger.error(f"í† í° ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            raise Exception(f"í† í° ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
    
    def detailed_similarity_analysis(self, text1: str, text2: str) -> Dict:
        """
        ë‘ í…ìŠ¤íŠ¸ì˜ ìƒì„¸í•œ ìœ ì‚¬ë„ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            text1 (str): ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸
            text2 (str): ë‘ ë²ˆì§¸ í…ìŠ¤íŠ¸
            
        Returns:
            Dict: ìƒì„¸ ë¶„ì„ ê²°ê³¼
        """
        try:
            # ê° í…ìŠ¤íŠ¸ì˜ í† í° ë¶„ì„
            token_analysis1 = self.analyze_text_tokens(text1)
            token_analysis2 = self.analyze_text_tokens(text2)
            
            # ê³µí†µ í† í° ì°¾ê¸°
            tokens1_set = set(token_analysis1['actual_tokens'])
            tokens2_set = set(token_analysis2['actual_tokens'])
            common_tokens = tokens1_set.intersection(tokens2_set)
            
            # ì„ë² ë”© ê³„ì‚°
            embedding1 = self.vectorize_text(text1)
            embedding2 = self.vectorize_text(text2)
            
            # ìœ ì‚¬ë„ ê³„ì‚°
            cosine_sim = self.cosine_similarity(embedding1, embedding2)
            
            # ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê³„ì‚°
            euclidean_distance = np.linalg.norm(embedding1 - embedding2)
            
            # ë§¨í•˜íƒ„ ê±°ë¦¬ ê³„ì‚°
            manhattan_distance = np.sum(np.abs(embedding1 - embedding2))
            
            # ì„ë² ë”© í†µê³„
            embedding1_stats = {
                'mean': float(np.mean(embedding1)),
                'std': float(np.std(embedding1)),
                'min': float(np.min(embedding1)),
                'max': float(np.max(embedding1)),
                'norm': float(np.linalg.norm(embedding1))
            }
            
            embedding2_stats = {
                'mean': float(np.mean(embedding2)),
                'std': float(np.std(embedding2)),
                'min': float(np.min(embedding2)),
                'max': float(np.max(embedding2)),
                'norm': float(np.linalg.norm(embedding2))
            }
            
            return {
                'text1': text1,
                'text2': text2,
                'token_analysis': {
                    'text1_tokens': token_analysis1['actual_tokens'],
                    'text2_tokens': token_analysis2['actual_tokens'],
                    'common_tokens': list(common_tokens),
                    'common_token_count': len(common_tokens),
                    'text1_unique_tokens': list(tokens1_set - tokens2_set),
                    'text2_unique_tokens': list(tokens2_set - tokens1_set),
                    'token_overlap_ratio': len(common_tokens) / max(len(tokens1_set), len(tokens2_set)) if max(len(tokens1_set), len(tokens2_set)) > 0 else 0
                },
                'similarity_metrics': {
                    'cosine_similarity': float(cosine_sim),
                    'euclidean_distance': float(euclidean_distance),
                    'manhattan_distance': float(manhattan_distance)
                },
                'embedding_stats': {
                    'text1': embedding1_stats,
                    'text2': embedding2_stats
                },
                'analysis': {
                    'embedding_dimension': len(embedding1),
                    'model_name': self.model_name
                }
            }
            
        except Exception as e:
            logger.error(f"ìƒì„¸ ìœ ì‚¬ë„ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            raise Exception(f"ìƒì„¸ ìœ ì‚¬ë„ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")

def handle_medical_symptom_test_interactive(vectorizer):
    """ì§ˆë¬¸-ë‹µë³€ì‹ ì˜ë£Œ ì¦ìƒ í…ŒìŠ¤íŠ¸ (í‘œ ê¸°ë°˜ ì§„ë‹¨)"""
    print("\nğŸ¥ ì˜ë£Œ ì¦ìƒ í…ŒìŠ¤íŠ¸ (ì§ˆë¬¸-ë‹µë³€ì‹)")
    print("-" * 40)
    
    # í‘œ ê¸°ë°˜ ì§„ë‹¨ ë£° ì •ì˜ (ê° ì—´ì´ í•˜ë‚˜ì˜ ì§ˆí™˜)
    disease_rules = [
        {
            'name': 'ì•ˆê²€ì—¼',
            'ë¶„ë¹„ë¬¼ íŠ¹ì„±': 'ë¹„ëŠ˜ ê°™ì€ ê°ì§ˆ, ê¸°ë¦„ê¸° ìˆëŠ” ë¶„ë¹„ë¬¼',
            'ì§„í–‰ ì†ë„': 'ì ì§„ì , ë§Œì„±ì ',
            'ì£¼ìš” ì¦ìƒ': 'ì†ëˆˆì¹ ì£¼ë³€ ê°ì§ˆ, ëˆˆêº¼í’€ ë¶™ìŒ',
            'ë°œìƒ íŒ¨í„´': 'ì–‘ì•ˆ'
        },
        {
            'name': 'ë¹„ê¶¤ì–‘ì„± ê°ë§‰ì—¼',
            'ë¶„ë¹„ë¬¼ íŠ¹ì„±': 'ë¯¸ì„¸í•œ ë¶„ë¹„ë¬¼, ì£¼ë¡œ ëˆˆë¬¼',
            'ì§„í–‰ ì†ë„': 'ì ì§„ì ',
            'ì£¼ìš” ì¦ìƒ': 'ëˆˆ ëœ¨ëŠ”ë° ì–´ë ¤ì›€ ì—†ìŒ, ê°ë§‰ í‘œë©´ ë§¤ë„ëŸ¬ì›€, ëˆˆë¬¼ í˜ë¦¼',
            'ë°œìƒ íŒ¨í„´': 'ë‹¨ì•ˆ ë˜ëŠ” ì–‘ì•ˆ'
        },
        {
            'name': 'ê²°ë§‰ì—¼',
            'ë¶„ë¹„ë¬¼ íŠ¹ì„±': 'ìˆ˜ì–‘ì„±, ì ì•¡ì„±, í™”ë†ì„±',
            'ì§„í–‰ ì†ë„': 'ê¸‰ì„±, ì ì§„ì ',
            'ì£¼ìš” ì¦ìƒ': 'ëˆˆì„ ë¹„ë¹„ëŠ” í–‰ë™, ì¶©í˜ˆ, ë¶€ì¢…',
            'ë°œìƒ íŒ¨í„´': 'ì–‘ì•ˆ(ì•Œë ˆë¥´ê¸°ì„±), ë‹¨ì•ˆ(ê°ì—¼ì„±)'
        },
        {
            'name': 'ê°ë§‰ê¶¤ì–‘',
            'ë¶„ë¹„ë¬¼ íŠ¹ì„±': 'í™”ë†ì„±, ì ì•¡ì„± ë¶„ë¹„ë¬¼',
            'ì§„í–‰ ì†ë„': 'ê¸‰ì„±, ë¹ ë¥¸ ì§„í–‰',
            'ì£¼ìš” ì¦ìƒ': 'ëˆˆì„ ëœ¨ê¸° í˜ë“¦, ëˆˆë¬¼, ì‹¬í•œ í†µì¦, ì‹œë ¥ ì €í•˜',
            'ë°œìƒ íŒ¨í„´': 'ë‹¨ì•ˆ'
        },
        {
            'name': 'ê°ë§‰ë¶€ê³¨í¸',
            'ë¶„ë¹„ë¬¼ íŠ¹ì„±': 'ìˆ˜ì–‘ì„±, ê²½ë¯¸í•œ ë¶„ë¹„ë¬¼',
            'ì§„í–‰ ì†ë„': 'ê¸‰ì„±, ë°˜ë³µì„±',
            'ì£¼ìš” ì¦ìƒ': 'ì•„ì¹¨ì— ì‹¬í•¨, ê°‘ì‘ìŠ¤ëŸ¬ìš´ ì‹¬í•œ í†µì¦, ì´ë¬¼ê°, ê¹œë¹¡ì„ ì‹œ í†µì¦',
            'ë°œìƒ íŒ¨í„´': 'ë‹¨ì•ˆ ë˜ëŠ” ì–‘ì•ˆ'
        }
    ]
    
    # ì§ˆë¬¸ ìˆœì„œ ë° í‚¤
    questions = [
        ('ë¶„ë¹„ë¬¼ íŠ¹ì„±', 'ë¶„ë¹„ë¬¼ì˜ íŠ¹ì„±(ì˜ˆ: ë¹„ëŠ˜ ê°™ì€ ê°ì§ˆ, ë¯¸ì„¸í•œ ë¶„ë¹„ë¬¼, ìˆ˜ì–‘ì„±/ì ì•¡ì„±/í™”ë†ì„± ë“±)ì„ ì…ë ¥í•˜ì„¸ìš”:'),
        ('ì§„í–‰ ì†ë„', 'ì§„í–‰ ì†ë„(ì˜ˆ: ì ì§„ì , ë§Œì„±ì , ê¸‰ì„± ë“±)ì„ ì…ë ¥í•˜ì„¸ìš”:'),
        ('ì£¼ìš” ì¦ìƒ', 'ì£¼ìš” ì¦ìƒ(ì˜ˆ: ê°ì§ˆ, ëˆˆêº¼í’€, ëˆˆ ëœ¨ê¸° ì–´ë ¤ì›€, ì¶©í˜ˆ ë“±)ì„ ì…ë ¥í•˜ì„¸ìš”:'),
        ('ë°œìƒ íŒ¨í„´', 'ë°œìƒ íŒ¨í„´(ì˜ˆ: ì–‘ì•ˆ, ë‹¨ì•ˆ, ì–‘ì•ˆ(ì•Œë ˆë¥´ê¸°ì„±), ë‹¨ì•ˆ(ê°ì—¼ì„±) ë“±)ì„ ì…ë ¥í•˜ì„¸ìš”:')
    ]
    
    # ì‚¬ìš©ì ë‹µë³€ ìˆ˜ì§‘
    user_answers = {}
    for key, question in questions:
        while True:
            answer = input(f"{question} ").strip()
            if answer:
                user_answers[key] = answer
                break
            else:
                print("âŒ ì…ë ¥ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    
    print("\nâš¡ ì…ë ¥í•˜ì‹  ì¦ìƒ ì •ë³´:")
    for k, v in user_answers.items():
        print(f"   {k}: {v}")
    print("\ní‘œ ê¸°ë°˜ ì§„ë‹¨ ë£°ê³¼ ë¹„êµ ì¤‘...")
    
    # ê° ì§ˆí™˜ë³„ë¡œ í•­ëª©ë³„ ìœ ì‚¬ë„ ê³„ì‚° ë° í•©ì‚°
    results = []
    for rule in disease_rules:
        total_score = 0
        detail_scores = {}
        for key in user_answers:
            user_text = user_answers[key]
            rule_text = rule[key]
            sim = vectorizer.cosine_similarity(
                vectorizer.vectorize_text(user_text),
                vectorizer.vectorize_text(rule_text)
            )
            detail_scores[key] = sim
            total_score += sim
        avg_score = total_score / len(user_answers)
        results.append({
            'disease': rule['name'],
            'avg_score': avg_score,
            'detail_scores': detail_scores
        })
    
    # ìœ ì‚¬ë„ ìˆœ ì •ë ¬
    results.sort(key=lambda x: x['avg_score'], reverse=True)
    
    print("\nâœ¨ ì§„ë‹¨ ê²°ê³¼ (ìœ ì‚¬ë„ ìˆœ):")
    for i, res in enumerate(results, 1):
        emoji = "ğŸ”¥" if i == 1 and res['avg_score'] >= 0.6 else ("âœ…" if res['avg_score'] >= 0.5 else "ğŸŸ¡")
        print(f"  {i}. {emoji} {res['disease']} (í‰ê·  ìœ ì‚¬ë„: {res['avg_score']:.3f})")
        for k, v in res['detail_scores'].items():
            print(f"     - {k} ìœ ì‚¬ë„: {v:.3f}")
    
    # ìµœì¢… ì¶”ì²œ
    top = results[0]
    if top['avg_score'] >= 0.6:
        print(f"\nğŸ’¡ ìµœì¢… ì§„ë‹¨ ì œì•ˆ: '{top['disease']}' ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.")
    elif top['avg_score'] >= 0.5:
        print(f"\nğŸ’¡ ìµœì¢… ì§„ë‹¨ ì œì•ˆ: '{top['disease']}'ì™€ ìœ ì‚¬í•˜ì§€ë§Œ ì¶”ê°€ ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    else:
        print(f"\nğŸ’¡ ìµœì¢… ì§„ë‹¨ ì œì•ˆ: ëª…í™•í•œ ì§„ë‹¨ì´ ì–´ë ¤ìš°ë‹ˆ ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    print("\n(ì°¸ê³ : ì´ ê²°ê³¼ëŠ” AI ê¸°ë°˜ ì°¸ê³ ìš©ì…ë‹ˆë‹¤. ì „ë¬¸ì˜ ìƒë‹´ì„ ê¶Œì¥í•©ë‹ˆë‹¤.)")

def handle_rule_embedding(vectorizer):
    """ì§„ë‹¨ ë£° ì„ë² ë”© ìƒì„± ë° ì €ì¥"""
    print("\nğŸ§  ì§„ë‹¨ ë£° ì„ë² ë”© ìƒì„±")
    print("-" * 40)
    
    # ì—…ë°ì´íŠ¸ëœ ì§„ë‹¨ ë£° ì •ì˜
    disease_rules = [
        {
            'name': 'ì•ˆê²€ì—¼',
            'ë¶„ë¹„ë¬¼ íŠ¹ì„±': 'ë¹„ëŠ˜ ê°™ì€ ê°ì§ˆ, ê¸°ë¦„ê¸° ìˆëŠ” ë¶„ë¹„ë¬¼',
            'ì§„í–‰ ì†ë„': 'ì ì§„ì , ë§Œì„±ì ',
            'ì£¼ìš” ì¦ìƒ': 'ì†ëˆˆì¹ ì£¼ë³€ ê°ì§ˆ, ëˆˆêº¼í’€ ë¶™ìŒ',
            'ë°œìƒ íŒ¨í„´': 'ì–‘ì•ˆ'
        },
        {
            'name': 'ë¹„ê¶¤ì–‘ì„± ê°ë§‰ì—¼',
            'ë¶„ë¹„ë¬¼ íŠ¹ì„±': 'ë¯¸ì„¸í•œ ë¶„ë¹„ë¬¼, ì£¼ë¡œ ëˆˆë¬¼',
            'ì§„í–‰ ì†ë„': 'ì ì§„ì ',
            'ì£¼ìš” ì¦ìƒ': 'ëˆˆ ëœ¨ëŠ”ë° ì–´ë ¤ì›€ ì—†ìŒ, ê°ë§‰ í‘œë©´ ë§¤ë„ëŸ¬ì›€, ëˆˆë¬¼ í˜ë¦¼',
            'ë°œìƒ íŒ¨í„´': 'ë‹¨ì•ˆ ë˜ëŠ” ì–‘ì•ˆ'
        },
        {
            'name': 'ê²°ë§‰ì—¼',
            'ë¶„ë¹„ë¬¼ íŠ¹ì„±': 'ìˆ˜ì–‘ì„±, ì ì•¡ì„±, í™”ë†ì„±',
            'ì§„í–‰ ì†ë„': 'ê¸‰ì„±, ì ì§„ì ',
            'ì£¼ìš” ì¦ìƒ': 'ëˆˆì„ ë¹„ë¹„ëŠ” í–‰ë™, ì¶©í˜ˆ, ë¶€ì¢…',
            'ë°œìƒ íŒ¨í„´': 'ì–‘ì•ˆ(ì•Œë ˆë¥´ê¸°ì„±), ë‹¨ì•ˆ(ê°ì—¼ì„±)'
        },
        {
            'name': 'ê°ë§‰ê¶¤ì–‘',
            'ë¶„ë¹„ë¬¼ íŠ¹ì„±': 'í™”ë†ì„±, ì ì•¡ì„± ë¶„ë¹„ë¬¼',
            'ì§„í–‰ ì†ë„': 'ê¸‰ì„±, ë¹ ë¥¸ ì§„í–‰',
            'ì£¼ìš” ì¦ìƒ': 'ëˆˆì„ ëœ¨ê¸° í˜ë“¦, ëˆˆë¬¼, ì‹¬í•œ í†µì¦, ì‹œë ¥ ì €í•˜',
            'ë°œìƒ íŒ¨í„´': 'ë‹¨ì•ˆ'
        },
        {
            'name': 'ê°ë§‰ë¶€ê³¨í¸',
            'ë¶„ë¹„ë¬¼ íŠ¹ì„±': 'ìˆ˜ì–‘ì„±, ê²½ë¯¸í•œ ë¶„ë¹„ë¬¼',
            'ì§„í–‰ ì†ë„': 'ê¸‰ì„±, ë°˜ë³µì„±',
            'ì£¼ìš” ì¦ìƒ': 'ì•„ì¹¨ì— ì‹¬í•¨, ê°‘ì‘ìŠ¤ëŸ¬ìš´ ì‹¬í•œ í†µì¦, ì´ë¬¼ê°, ê¹œë¹¡ì„ ì‹œ í†µì¦',
            'ë°œìƒ íŒ¨í„´': 'ë‹¨ì•ˆ ë˜ëŠ” ì–‘ì•ˆ'
        }
    ]
    
    print(f"ğŸ“‹ ì´ {len(disease_rules)}ê°œì˜ ì§ˆë³‘ ë£°ì„ ì„ë² ë”©í•©ë‹ˆë‹¤...")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = "rule_embeddings"
    os.makedirs(output_dir, exist_ok=True)
    
    all_rule_embeddings = []
    
    # ê° ì§ˆë³‘ë³„ë¡œ ë£° ì„ë² ë”© ìƒì„±
    for disease in disease_rules:
        disease_name = disease['name']
        print(f"\nğŸ” {disease_name} ë£° ì„ë² ë”© ì¤‘...")
        
        disease_embeddings = {
            'disease_name': disease_name,
            'rule_embeddings': {},
            'combined_embedding': None
        }
        
        # ê° í•­ëª©ë³„ ì„ë² ë”© ìƒì„±
        rule_texts = []
        for category, description in disease.items():
            if category != 'name':
                rule_text = f"{category}: {description}"
                rule_texts.append(rule_text)
                
                # ê°œë³„ í•­ëª© ì„ë² ë”©
                embedding = vectorizer.vectorize_text(rule_text)
                disease_embeddings['rule_embeddings'][category] = {
                    'text': rule_text,
                    'embedding': embedding.tolist(),
                    'dimension': len(embedding)
                }
                print(f"   âœ… {category} ì„ë² ë”© ì™„ë£Œ (ì°¨ì›: {len(embedding)})")
        
        # ì „ì²´ ë£°ì„ í•©ì¹œ ì„ë² ë”© ìƒì„±
        combined_text = f"{disease_name}: " + ", ".join([f"{k}({v})" for k, v in disease.items() if k != 'name'])
        combined_embedding = vectorizer.vectorize_text(combined_text)
        disease_embeddings['combined_embedding'] = {
            'text': combined_text,
            'embedding': combined_embedding.tolist(),
            'dimension': len(combined_embedding)
        }
        print(f"   ğŸ”¥ {disease_name} í†µí•© ì„ë² ë”© ì™„ë£Œ (ì°¨ì›: {len(combined_embedding)})")
        
        # ê°œë³„ ì§ˆë³‘ íŒŒì¼ ì €ì¥
        disease_file = os.path.join(output_dir, f"{disease_name}_embeddings.json")
        with open(disease_file, 'w', encoding='utf-8') as f:
            json.dump(disease_embeddings, f, ensure_ascii=False, indent=2)
        
        all_rule_embeddings.append(disease_embeddings)
    
    # ì „ì²´ ë£° ì„ë² ë”© íŒŒì¼ ì €ì¥
    all_embeddings_data = {
        'model_name': vectorizer.model_name,
        'created_at': datetime.now().isoformat(),
        'total_diseases': len(disease_rules),
        'embedding_dimension': len(combined_embedding),
        'disease_embeddings': all_rule_embeddings
    }
    
    all_file = os.path.join(output_dir, "all_disease_embeddings.json")
    with open(all_file, 'w', encoding='utf-8') as f:
        json.dump(all_embeddings_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… ëª¨ë“  ë£° ì„ë² ë”© ì™„ë£Œ!")
    print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {output_dir}/")
    print(f"ğŸ“Š ì´ {len(disease_rules)}ê°œ ì§ˆë³‘, ì„ë² ë”© ì°¨ì›: {len(combined_embedding)}")
    print(f"ğŸ—ƒï¸ ê°œë³„ íŒŒì¼: {len(disease_rules)}ê°œ")
    print(f"ğŸ—‚ï¸ í†µí•© íŒŒì¼: all_disease_embeddings.json")
    
    # ì„ë² ë”© í†µê³„ ì •ë³´
    print(f"\nğŸ“ˆ ì„ë² ë”© í†µê³„:")
    for disease_data in all_rule_embeddings:
        disease_name = disease_data['disease_name']
        combined_emb = np.array(disease_data['combined_embedding']['embedding'])
        print(f"   {disease_name}:")
        print(f"     - í‰ê· : {np.mean(combined_emb):.6f}")
        print(f"     - í‘œì¤€í¸ì°¨: {np.std(combined_emb):.6f}")
        print(f"     - ë†ˆ: {np.linalg.norm(combined_emb):.6f}")
    
    return all_embeddings_data

def handle_detailed_analysis(vectorizer):
    """ë‘ í…ìŠ¤íŠ¸ ìƒì„¸ ë¶„ì„ ì²˜ë¦¬"""
    print("\nğŸ” ë‘ í…ìŠ¤íŠ¸ ìƒì„¸ ë¶„ì„ ëª¨ë“œ")
    print("-" * 40)
    
    while True:
        print("\n" + "-" * 40)
        print("ë‘ ê°œì˜ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”:")
        
        # ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ ì…ë ¥
        text1 = input("ğŸ“ ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ (ë’¤ë¡œê°€ê¸°: 'back'): ").strip()
        if text1.lower() == 'back':
            break
        
        if not text1:
            print("âŒ ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            continue
        
        # ë‘ ë²ˆì§¸ í…ìŠ¤íŠ¸ ì…ë ¥
        text2 = input("ğŸ“ ë‘ ë²ˆì§¸ í…ìŠ¤íŠ¸: ").strip()
        if not text2:
            print("âŒ ë‘ ë²ˆì§¸ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            continue
        
        print(f"âš¡ ë‘ í…ìŠ¤íŠ¸ë¥¼ ìƒì„¸ ë¶„ì„ ì¤‘...")
        
        try:
            # ìƒì„¸ ë¶„ì„
            analysis_result = vectorizer.detailed_similarity_analysis(text1, text2)
            
            # ê²°ê³¼ ì¶œë ¥
            print(f"\nâœ¨ ìƒì„¸ ë¶„ì„ ì™„ë£Œ!")
            print(f"ğŸ“Š ê²°ê³¼:")
            print(f"   í…ìŠ¤íŠ¸ 1: {text1}")
            print(f"   í…ìŠ¤íŠ¸ 2: {text2}")
            
            # í† í° ë¶„ì„ ê²°ê³¼
            print(f"\nğŸ“ˆ í† í° ë¶„ì„:")
            print(f"   í…ìŠ¤íŠ¸ 1 í† í°: {analysis_result['token_analysis']['text1_tokens']}")
            print(f"   í…ìŠ¤íŠ¸ 2 í† í°: {analysis_result['token_analysis']['text2_tokens']}")
            print(f"   ê³µí†µ í† í° ìˆ˜: {analysis_result['token_analysis']['common_token_count']}")
            print(f"   ê³µí†µ í† í°: {analysis_result['token_analysis']['common_tokens']}")
            print(f"   í† í° ê²¹ì¹¨ ë¹„ìœ¨: {analysis_result['token_analysis']['token_overlap_ratio']:.3f}")
            
            if analysis_result['token_analysis']['common_token_count'] > 0:
                print(f"\nğŸ” ì™œ ìœ ì‚¬ë„ê°€ ë†’ì€ì§€ ë¶„ì„:")
                print(f"   - ê³µí†µ í† í°ì´ {analysis_result['token_analysis']['common_token_count']}ê°œ ìˆìŠµë‹ˆë‹¤")
                print(f"   - ê³µí†µ í† í°: {', '.join(analysis_result['token_analysis']['common_tokens'])}")
                print(f"   - ì´ëŸ¬í•œ ê³µí†µ í† í°ë“¤ì´ ìœ ì‚¬ë„ë¥¼ ë†’ì´ëŠ” ì£¼ìš” ì›ì¸ìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤")
            
            # ê³ ìœ  í† í° ë¶„ì„
            print(f"\nğŸ”„ ê³ ìœ  í† í° ë¶„ì„:")
            print(f"   í…ìŠ¤íŠ¸ 1ë§Œì˜ í† í°: {analysis_result['token_analysis']['text1_unique_tokens']}")
            print(f"   í…ìŠ¤íŠ¸ 2ë§Œì˜ í† í°: {analysis_result['token_analysis']['text2_unique_tokens']}")
            
            # ìœ ì‚¬ë„ í•´ì„
            cosine_sim = analysis_result['similarity_metrics']['cosine_similarity']
            print(f"\nğŸ“Š ìœ ì‚¬ë„ í•´ì„:")
            print(f"   ì½”ì‚¬ì¸ ìœ ì‚¬ë„: {cosine_sim:.6f}")
            
            if cosine_sim >= 0.9:
                interpretation = "ë§¤ìš° ìœ ì‚¬í•¨ ğŸŸ¢"
            elif cosine_sim >= 0.7:
                interpretation = "ìœ ì‚¬í•¨ ğŸŸ¡"
            elif cosine_sim >= 0.5:
                interpretation = "ë³´í†µ ğŸŸ "
            elif cosine_sim >= 0.3:
                interpretation = "ë‹¤ì†Œ ë‹¤ë¦„ ğŸ”´"
            else:
                interpretation = "ë§¤ìš° ë‹¤ë¦„ âš«"
            
            print(f"   í•´ì„: {interpretation}")
            print(f"   ìœ í´ë¦¬ë“œ ê±°ë¦¬: {analysis_result['similarity_metrics']['euclidean_distance']:.6f}")
            print(f"   ë§¨í•˜íƒ„ ê±°ë¦¬: {analysis_result['similarity_metrics']['manhattan_distance']:.6f}")
            
            # ì„ë² ë”© í†µê³„
            print(f"\nğŸ“Š ì„ë² ë”© í†µê³„:")
            print(f"   í…ìŠ¤íŠ¸ 1 ì„ë² ë”© - í‰ê· : {analysis_result['embedding_stats']['text1']['mean']:.6f}, í‘œì¤€í¸ì°¨: {analysis_result['embedding_stats']['text1']['std']:.6f}")
            print(f"   í…ìŠ¤íŠ¸ 2 ì„ë² ë”© - í‰ê· : {analysis_result['embedding_stats']['text2']['mean']:.6f}, í‘œì¤€í¸ì°¨: {analysis_result['embedding_stats']['text2']['std']:.6f}")
        
        except Exception as e:
            print(f"âŒ ìƒì„¸ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")

def main():
    """ë©”ì¸ í•¨ìˆ˜ - ì§„ë‹¨ ë£° ì„ë² ë”© ìƒì„± ë° ì €ì¥"""
    print("ğŸ§  ì§„ë‹¨ ë£° ì„ë² ë”© ìƒì„± ë„êµ¬")
    print("=" * 60)
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    try:
        # ì˜ë£Œ íŠ¹í™” ëª¨ë¸ ì‚¬ìš© ì„¤ì •
        use_medical = True
        print("ğŸ“¥ KM-BERT (ì˜ë£Œ íŠ¹í™”) ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        # RuleVectorizer ì´ˆê¸°í™”
        vectorizer = RuleVectorizer(use_medical_model=use_medical)
        
        # ëª¨ë¸ ì •ë³´ ì¶œë ¥
        model_info = vectorizer.get_model_info()
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ:")
        print(f"   ëª¨ë¸ëª…: {model_info['model_name']}")
        print(f"   ë””ë°”ì´ìŠ¤: {model_info['device']}")
        print(f"   ì„ë² ë”© ì°¨ì›: {model_info['hidden_size']}")
        print()
        
        # ì§„ë‹¨ ë£° ì •ì˜
        disease_rules = [
            {
                'name': 'ì•ˆê²€ì—¼',
                'ë¶„ë¹„ë¬¼ íŠ¹ì„±': 'ë¹„ëŠ˜ ê°™ì€ ê°ì§ˆ, ê¸°ë¦„ê¸° ìˆëŠ” ë¶„ë¹„ë¬¼',
                'ì§„í–‰ ì†ë„': 'ì ì§„ì , ë§Œì„±ì ',
                'ì£¼ìš” ì¦ìƒ': 'ì†ëˆˆì¹ ì£¼ë³€ ê°ì§ˆ, ëˆˆêº¼í’€ ë¶™ìŒ',
                'ë°œìƒ íŒ¨í„´': 'ì–‘ì•ˆ'
            },
            {
                'name': 'ë¹„ê¶¤ì–‘ì„± ê°ë§‰ì—¼',
                'ë¶„ë¹„ë¬¼ íŠ¹ì„±': 'ë¯¸ì„¸í•œ ë¶„ë¹„ë¬¼, ì£¼ë¡œ ëˆˆë¬¼',
                'ì§„í–‰ ì†ë„': 'ì ì§„ì ',
                'ì£¼ìš” ì¦ìƒ': 'ëˆˆ ëœ¨ëŠ”ë° ì–´ë ¤ì›€ ì—†ìŒ, ê°ë§‰ í‘œë©´ ë§¤ë„ëŸ¬ì›€, ëˆˆë¬¼ í˜ë¦¼',
                'ë°œìƒ íŒ¨í„´': 'ë‹¨ì•ˆ ë˜ëŠ” ì–‘ì•ˆ'
            },
            {
                'name': 'ê²°ë§‰ì—¼',
                'ë¶„ë¹„ë¬¼ íŠ¹ì„±': 'ìˆ˜ì–‘ì„±, ì ì•¡ì„±, í™”ë†ì„±',
                'ì§„í–‰ ì†ë„': 'ê¸‰ì„±, ì ì§„ì ',
                'ì£¼ìš” ì¦ìƒ': 'ëˆˆì„ ë¹„ë¹„ëŠ” í–‰ë™, ì¶©í˜ˆ, ë¶€ì¢…',
                'ë°œìƒ íŒ¨í„´': 'ì–‘ì•ˆ(ì•Œë ˆë¥´ê¸°ì„±), ë‹¨ì•ˆ(ê°ì—¼ì„±)'
            },
            {
                'name': 'ê°ë§‰ê¶¤ì–‘',
                'ë¶„ë¹„ë¬¼ íŠ¹ì„±': 'í™”ë†ì„±, ì ì•¡ì„± ë¶„ë¹„ë¬¼',
                'ì§„í–‰ ì†ë„': 'ê¸‰ì„±, ë¹ ë¥¸ ì§„í–‰',
                'ì£¼ìš” ì¦ìƒ': 'ëˆˆì„ ëœ¨ê¸° í˜ë“¦, ëˆˆë¬¼, ì‹¬í•œ í†µì¦, ì‹œë ¥ ì €í•˜',
                'ë°œìƒ íŒ¨í„´': 'ë‹¨ì•ˆ'
            },
            {
                'name': 'ê°ë§‰ë¶€ê³¨í¸',
                'ë¶„ë¹„ë¬¼ íŠ¹ì„±': 'ìˆ˜ì–‘ì„±, ê²½ë¯¸í•œ ë¶„ë¹„ë¬¼',
                'ì§„í–‰ ì†ë„': 'ê¸‰ì„±, ë°˜ë³µì„±',
                'ì£¼ìš” ì¦ìƒ': 'ì•„ì¹¨ì— ì‹¬í•¨, ê°‘ì‘ìŠ¤ëŸ¬ìš´ ì‹¬í•œ í†µì¦, ì´ë¬¼ê°, ê¹œë¹¡ì„ ì‹œ í†µì¦',
                'ë°œìƒ íŒ¨í„´': 'ë‹¨ì•ˆ ë˜ëŠ” ì–‘ì•ˆ'
            }
        ]
        
        print(f"ğŸ“‹ ì´ {len(disease_rules)}ê°œì˜ ì§ˆë³‘ ë£°ì„ ì„ë² ë”©í•©ë‹ˆë‹¤...")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        output_dir = "rule_embeddings"
        os.makedirs(output_dir, exist_ok=True)
        
        # ê° ì§ˆë³‘ë³„ë¡œ ë£° ì„ë² ë”© ìƒì„± ë° ì €ì¥
        for disease in disease_rules:
            disease_name = disease['name']
            print(f"\nğŸ” {disease_name} ë£° ì„ë² ë”© ì¤‘...")
            
            # ê° í•­ëª©ë³„ ì„ë² ë”© ìƒì„±
            for category, description in disease.items():
                if category != 'name':
                    rule_text = f"{category}: {description}"
                    
                    # ì„ë² ë”© ìƒì„±
                    embedding = vectorizer.vectorize_text(rule_text)
                    
                    # NumPy íŒŒì¼ë¡œ ì €ì¥
                    filename = f"{disease_name}_{category}.npy"
                    filepath = os.path.join(output_dir, filename)
                    np.save(filepath, embedding)
                    
                    print(f"   âœ… {category} ì„ë² ë”© ì €ì¥: {filename} (ì°¨ì›: {len(embedding)})")
            
            # ì „ì²´ ë£°ì„ í•©ì¹œ ì„ë² ë”© ìƒì„±
            combined_text = f"{disease_name}: " + ", ".join([f"{k}({v})" for k, v in disease.items() if k != 'name'])
            combined_embedding = vectorizer.vectorize_text(combined_text)
            
            # í†µí•© ì„ë² ë”©ë„ NumPy íŒŒì¼ë¡œ ì €ì¥
            combined_filename = f"{disease_name}_combined.npy"
            combined_filepath = os.path.join(output_dir, combined_filename)
            np.save(combined_filepath, combined_embedding)
            
            print(f"   ğŸ”¥ {disease_name} í†µí•© ì„ë² ë”© ì €ì¥: {combined_filename} (ì°¨ì›: {len(combined_embedding)})")
        
        print(f"\nâœ… ëª¨ë“  ë£° ì„ë² ë”© ì™„ë£Œ!")
        print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {output_dir}/")
        print(f"ğŸ“Š ì´ {len(disease_rules)}ê°œ ì§ˆë³‘")
        print(f"ğŸ“ ì´ {len(disease_rules) * 5}ê°œ ì„ë² ë”© íŒŒì¼ ìƒì„± (.npy í˜•ì‹)")
        print(f"ğŸ‰ ì„ë² ë”© ìƒì„± ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        
    except KeyboardInterrupt:
        print("\nğŸ‘‹ í”„ë¡œê·¸ë¨ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 